@retry(wait=wait_random_exponential(min=45, max=120), stop=stop_after_attempt(6))
def query_llm(messages, temperature=0.0, max_tokens=4096):
    print("\n" + "="*80)
    print("CALLING AZURE OPENAI")
    print("="*80)
    print(f"MODEL          : {model_name}")
    print(f"TEMPERATURE    : {temperature}")
    print(f"MAX_TOKENS     : {max_tokens}")
    print(f"STREAM         : False")
    print(f"TIMEOUT        : 300")
    print(f"MESSAGES COUNT : {len(messages)}")
    print("-" * 80)
    
    for i, msg in enumerate(messages):
        role = msg["role"].upper()
        content_preview = str(msg["content"])[:500]
        if len(str(msg["content"])) > 500:
            content_preview += " [...]"
        print(f"{role} [{i}]: {content_preview}")
    print("="*80)

    try:
        response = chat_client.chat.completions.create(
            messages=messages,
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=1.0,
            stream=False,
            timeout=300
        )
        content = response.choices[0].message.content
        if content is None:
            content = "[EMPTY CONTENT RETURNED]"
        
        print("RESPONSE RECEIVED")
        print(f"Content length: {len(content)} characters")
        print("-" * 80)
        print(content[:1000])
        if len(content) > 1000:
            print("... [truncated]")
        print("="*80 + "\n")
        
        return content.strip()
        
    except Exception as e:
        print(f"EXCEPTION IN query_llm: {type(e).__name__}: {e}")
        print("="*80 + "\n")
        return "[query_llm failed â€“ routed for review]")
